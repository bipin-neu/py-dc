{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8ad4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The __init__.py file lets python know that a directory is a package.\n",
    "# https://peps.python.org/pep-0008/#package-and-module-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ea607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import text_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ae1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = [Counter({'H': 2, 'O': 2, 'DataCamp': 1, 'Introduction': 1, 'to': 1, 'AutoML': 1, 'In': 1, 'this': 1, 'tutorial': 1, 'you': 1, 'will': 1, 'learn': 1, 'about': 1, 'and': 1, 'have': 1, 'a': 1, 'glimpse': 1, 'of': 1, 'its': 1, 'auto': 1}), Counter({'DataCamp': 1, 'Stocks': 1, 'Significance': 1, 'Testing': 1, 'p': 1, 'Hacking': 1, 'Learn': 1, 'how': 1, 'to': 1, 'manipulate': 1, 'time': 1, 'series': 1, 'data': 1, 'with': 1, 'pandas': 1, 'and': 1, 'co': 1}), Counter({'RT': 1, '@cbismuth': 1, 'Linear': 1, 'regression': 1, 'example': 1, 'with': 1, 'most': 1, 'significant': 1, 'features': 1, 'detection': 1, '#DataCamp': 1, '#DataScience': 1, '#Python': 1, '#sklearn': 1}), Counter({'Linear': 1, 'regression': 1, 'example': 1, 'with': 1, 'most': 1, 'significant': 1, 'features': 1, 'detection': 1, '#DataCamp': 1, '#DataScience': 1, '#Python': 1, '#sklearn': 1})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad75e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('with', 3), ('H', 2), ('O', 2), ('DataCamp', 2), ('to', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Sum word_counts using sum_counters from text_analyzer\n",
    "word_count_totals = text_analyzer.sum_counters(word_counts)\n",
    "\n",
    "# Plot word_count_totals using plot_counter from text_analyzer\n",
    "text_analyzer.plot_counter(word_count_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893363a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datacamp_tweet\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Document with datacamp_tweet\n",
    "my_document = text_analyzer.Document(text='datacamp_tweet')\n",
    "\n",
    "# Print the text attribute of the Document instance\n",
    "print(my_document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e54298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_count_words',\n",
       " '_tokenize',\n",
       " 'text',\n",
       " 'tokens',\n",
       " 'word_counts']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#know all attrib, func defined - see at end of dir output\n",
    "dir(my_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c3d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _tokenize in module text_analyzer.document:\n",
      "\n",
      "_tokenize() method of text_analyzer.document.Document instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(my_document._tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fe38a",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbfafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inline comments - comment 'why' rather than 'what'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65931d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our competitor pricing is $10.50 an inch. Our price is $125.00 a foot.\n",
      "['$10.50', '$125.00']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Our competitor pricing is $10.50 an inch. Our price is $125.00 a foot.\"\n",
    "def extract_0(text):\n",
    "    # match and extract dollar amounts from the text\n",
    "    return re.findall(r'\\$\\d+\\.\\d\\d', text)\n",
    "\n",
    "def extract_1(text):\n",
    "    # return all matches to regex pattern\n",
    "    return re.findall(r'\\$\\d+\\.\\d\\d', text)\n",
    "\n",
    "# Print the text\n",
    "print(text)\n",
    "\n",
    "# Print the results of the function with better commenting\n",
    "print(extract_0(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5241414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tokenize in module __main__:\n",
      "\n",
      "tokenize(text, regex='[a-zA-z]+')\n",
      "    Split text into tokens using a regular expression\n",
      "\n",
      "    :param text: text to be tokenized\n",
      "    :param regex: regular expression used to match tokens using re.findall\n",
      "    :return: a list of resulting tokens\n",
      "\n",
      "    >>> tokenize('the rain in spain')\n",
      "    ['the', 'rain', 'in', 'spain']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete the function's docstring\n",
    "def tokenize(text, regex=r'[a-zA-z]+'):\n",
    "  \"\"\"Split text into tokens using a regular expression\n",
    "\n",
    "  :param text: text to be tokenized\n",
    "  :param regex: regular expression used to match tokens using re.findall \n",
    "  :return: a list of resulting tokens\n",
    "\n",
    "  >>> tokenize('the rain in spain')\n",
    "  ['the', 'rain', 'in', 'spain']\n",
    "  \"\"\"\n",
    "  return re.findall(regex, text, flags=re.IGNORECASE)\n",
    "\n",
    "# Print the docstring\n",
    "help(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a927aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 10, in __main__.sum_counters\n",
      "Failed example:\n",
      "    sum_counters([d1.word_counts, d2.word_counts])\n",
      "Expected:\n",
      "    Counter({'fizz': 4, 'buzz': 2})\n",
      "Got:\n",
      "    Counter({'fizz': 4, 'buzz': 2, '1': 1, '2': 1, '4': 1, '7': 1, '8': 1, '11': 1, '13': 1, '14': 1})\n",
      "\u001b[31m**********************************************************************\u001b[0m\n",
      "1 item had failures:\n",
      "   1 of   3 in __main__.sum_counters\n",
      "\u001b[1;31m***Test Failed*** 1 failure\u001b[0m.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8960831a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SocialMedia' from 'text_analyzer' (/Users/bipin/CodeGround/IntellijProjects/py-dc/track/text_analyzer/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_analyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SocialMedia\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create an instance of SocialMedia for testing\u001b[39;00m\n\u001b[32m      5\u001b[39m test_post = \u001b[33m'\u001b[39m\u001b[33mlearning #python & #rstats is awesome! thanks @datacamp!\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SocialMedia' from 'text_analyzer' (/Users/bipin/CodeGround/IntellijProjects/py-dc/track/text_analyzer/__init__.py)"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from text_analyzer import SocialMedia\n",
    "\n",
    "# Create an instance of SocialMedia for testing\n",
    "test_post = 'learning #python & #rstats is awesome! thanks @datacamp!'\n",
    "sm_post = SocialMedia(test_post)\n",
    "\n",
    "# Test hashtag counts are created properly\n",
    "def test_social_media_hashtags():\n",
    "    expected_hashtag_counts = Counter({'#python': 1, '#rstats': 1})\n",
    "    assert sm_post.hashtag_counts == expected_hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6456a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 10, in __main__.sum_counters\n",
      "Failed example:\n",
      "    sum_counters([d1.word_counts, d2.word_counts])\n",
      "Expected:\n",
      "    Counter({'fizz': 4, 'buzz': 2})\n",
      "Got:\n",
      "    Counter({'fizz': 4, 'buzz': 2, '1': 1, '2': 1, '4': 1, '7': 1, '8': 1, '11': 1, '13': 1, '14': 1})\n",
      "\u001b[31m**********************************************************************\u001b[0m\n",
      "1 item had failures:\n",
      "   1 of   3 in __main__.sum_counters\n",
      "\u001b[1;31m***Test Failed*** 1 failure\u001b[0m.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text_analyzer\n",
    "def sum_counters(counters):\n",
    "    \"\"\"Aggregate collections.Counter objects by summing counts\n",
    "\n",
    "    :param counters: list/tuple of counters to sum\n",
    "    :return: aggregated counters with counts summed\n",
    "\n",
    "    >>> d1 = text_analyzer.Document('1 2 fizz 4 buzz fizz 7 8')\n",
    "    >>> d2 = text_analyzer.Document('fizz buzz 11 fizz 13 14')\n",
    "    >>> sum_counters([d1.word_counts, d2.word_counts])\n",
    "    Counter({'fizz': 4, 'buzz': 2})\n",
    "    \"\"\"\n",
    "    return sum(counters, Counter())\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3439f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from text_analyzer import SocialMedia\n",
    "\n",
    "# Create an instance of SocialMedia for testing\n",
    "test_post = 'learning #python & #rstats is awesome! thanks @datacamp!'\n",
    "sm_post = SocialMedia(test_post)\n",
    "\n",
    "# Test hashtag counts are created properly\n",
    "def test_social_media_hashtags():\n",
    "    expected_hashtag_counts = Counter({'#python': 1, '#rstats': 1})\n",
    "    assert sm_post.hashtag_counts == expected_hashtag_counts\n",
    "\n",
    "#pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b905c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Sphinx to generate documentation from code\n",
    "#CodeCov - where to improve project tests\n",
    "# codeclimate - Analyze code for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e648c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-dc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
